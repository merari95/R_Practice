---
title: "DSCI 523 Lab 3"
subtitle: "Tidy control flow in R, as well as functions & testing in R"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lab Mechanics

rubric={mechanics:5}

-   All files necessary to run your work must be pushed to your
    GitHub.ubc.ca repository for this lab.
-   You need to have a minimum of 3 commit messages associated with your
    GitHub.ubc.ca repository for this lab.
-   You must also submit `.Rmd` file and the rendered pdf in this
    worksheet/lab to Gradescope. Entire notebook must be executed so the
    TA's can see the results of your work.
-   **There is autograding in this lab, so please do not move or rename
    this file. Also, do not copy and paste cells, if you need to add new
    cells, create new cells via the "Insert a cell below" button
    instead.**
-   To ensure you do not break the autograder remove all code for
    installing packages (i.e., DO NOT have `install.packages(...)` or
    `devtools::install_github(...)` in your homework!
-   Follow the [MDS general lab
    instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).
-   <mark>This lab has hidden tests. In this lab, the visible tests are
    just there to ensure you create an object with the correct name. The
    remaining tests are hidden intentionally. This is so you get
    practice deciding when you have written the correct code and created
    the correct data object. This is a necessary skill for data
    scientists, and if we were to provide robust visible tests for all
    questions you would not develop this skill, or at least not to its
    full potential.</mark>

## Code Quality

rubric={quality:5}

The code that you write for this assignment will be given one overall
grade for code quality, see our code quality rubric as a guide to what
we are looking for. Also, for this course (and other MDS courses that
use R), we are trying to follow the tidyverse code style. There is a
guide you can refer too: <http://style.tidyverse.org/>

Each code question will also be assessed for code accuracy (i.e., does
it do what it is supposed to do?).

## Writing

rubric={writing:5}

To get the marks for this writing component, you should:

-   Use proper English, spelling, and grammar throughout your submission
    (the non-coding parts).
-   Be succinct. This means being specific about what you want to
    communicate, without being superfluous.

## Let's get started!

Run the cell below to load the libraries needed for this lab, as well as
the test file so you can check your answers as you go!

```{r}
library(nycflights13)
library(janitor)
library(testthat)
library(readr)
library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
```

> Note - there is an issue with loading packages with `tidyverse` and
> the autograding software we are using. Thus, for assignments, please
> load the packages individually as I have done above instead of loading
> them via the tidyverse.

## Exercise 1: control flow with {dplyr}

rubric={autograde=15}

Use the {tidyverse} control flow functions we learned about this week to
take the {nycflights13} `flights` data set and obtain the average speed
(in km/hr) and average distance (in km) for the carriers AA, AS, UA and
US. Name these new columns `avg_speed` and `avg_distance_km`, and round
the values so that the answer is a whole number (i.e., no decimal
points). Convert the carrier acronyms to their full names (American
Airlines, Alaska Airlines, United Airlines and US Airways). Sort the
results according to the average speed. Bind the name `avg_flights` to
the data frame.

Some hints: - The distance is in miles and air time is in minutes in the
`flights` data. - You will have to create a column that holds the
average speed for each flight before you can do this for each carrier. -
You may also need to handle `NA` entries in the data.

```{r tags=c()}
#the goal is to transform the dataframe so that only the carriers AA, AS, UA and US are present. Change them to their original carrier names. Calculate the average speed and distance per carrier

flights
avg_flights <- flights |> #creating new dataframe avg_flights using flights dataframe
  filter(carrier %in% c("AA", "AS", "UA", "US"))|> #filter for variables in carrier column when it is equal to the following names
  mutate(distance_km = distance * 1.60934, #convert the distance column to be in km by creating new distance_km column
         time_hr = air_time/60) |> #convert the air_time column to be in hours by creating time_hr column
  select(carrier, distance_km, time_hr) |> #select only these columns in the dataframe
  drop_na() |> #drop any row with NA values
  group_by(carrier) |> #group the data based on their carrier
  summarize(avg_distance_km = mean(distance_km), #take the mean of the distance per carrier
            avg_speed = mean(distance_km/time_hr)) |> #take the mean of the speed per carrier
  mutate(avg_distance_km = round(avg_distance_km), #round the columns so they are whole numbers
         avg_speed = round(avg_speed)) |> 
  mutate(carrier = case_when(#change the carrier variables to their original long name
    carrier == "AA" ~ "American Airlines",
    carrier == "AS" ~ "Alaska Airlines",
    carrier == "UA" ~ "United Airlines",
    carrier == "US" ~ "US Airways",
    TRUE ~ carrier        # fallback for unexpected codes #if there is a carrier not mentioned above, leave the name as is
  )) |> 
  arrange(avg_speed) #arrange the data by ascending order of the average speed of the carriers

avg_flights
```

The tests below only check that the object has the correct names. The
other tests are intentionally hidden.

```{r}
. = ottr::check("tests/e1.R")
```

## Exercise 2: mapping with {purrr}

rubric={accuracy:20}

We want to know if the list mixed_bag given below contains all numeric
elements, if it does, we want to output `TRUE`. If not, we want to
output `FALSE`.

To do this use a {purrr} `map*` function to iterate over the list given
below to generate a logical vector that holds `TRUE` if the list element
is numeric and `FALSE` if it is not. Then use the fact that R can sum
logical vectors (`TRUE` takes on the value of 1 and `FALSE` takes on the
value of 0) and check whether the sum of the logical vector generated by
map equals the length of the mixed_bag list.

```{r tags=c()}
mixed_bag <- list(c(11232, 21231, 32123),
                 "https://github.com/UBC-DSCI/introduction-to-datascience",
                 c(TRUE, FALSE, FALSE, TRUE, TRUE),
                 c("CRC Press"),
                 list(1, 2, 3))


# Step 1: Check if each element is numeric
logical_vec <- map_lgl(mixed_bag, is.numeric)

# Step 2: Compare the sum of TRUEs to the length of the list
all_numeric <- sum(logical_vec) == length(mixed_bag)

all_numeric
```

## Exercise 3: functions

rubric={accuracy:10, reasoning:6}

#### Part 1:

Consider the code below, where we are repeatedly reading in data from
Vancouver's Open Data portal, and cleaning the column names (so that no
column names are non-syntactic).

Your job is to take the redundant code and make it modular, by wrapping
it in a function. Your function should meet the following
specifications:

-   input: a URL to a `.csv` from Vancouver's Open Data portal
-   output: a tibble with syntactic column names

> If you feel inspired, feel free to make your function more flexible by
> using function arguments!

Your function should follow the tidyverse style guide, as well as be
well documented using `roxygen2` documentation. Finally, demonstrate
that your function works using an example.

#### Part 2:

In 2-3 sentences, and in your own words, also discuss why it would be
worth modularizing the code into a function if you were not repeating it
several times (i.e., you were just using it to read in the data once).

```{r}
# code to modularize
parks <- read_delim("https://opendata.vancouver.ca/api/explore/v2.1/catalog/datasets/parks/exports/csv?lang=en&timezone=America%2FLos_Angeles&use_labels=true&delimiter=%3B",
                        delim = ";")

art <- art_data <- read_delim("https://opendata.vancouver.ca/api/explore/v2.1/catalog/datasets/public-art/exports/csv?lang=en&timezone=America%2FLos_Angeles&use_labels=true&delimiter=%3B",
                       delim = ";")

clean_parks <- parks |> 
  clean_names()

clean_art <- art |> 
  clean_names()
```

*Type your answer here, replacing this text.*

```{r tags=c()}
#' Read and clean a Vancouver Open Data CSV
#'
#' This function reads a .csv file from Vancouver's Open Data portal
#' and returns a tibble with syntactic (clean) column names.
#'
#' @param url A character string giving the URL to a .csv file from
#'   Vancouver's Open Data portal.
#' @param delim A single character used as the column separator.
#'   Defaults to ";" for the Open Data portal exports.
#'
#' @return A tibble with cleaned, syntactic column names.
#'
#' @examples
#' parks_url <- "https://opendata.vancouver.ca/api/explore/v2.1/catalog/datasets/parks/exports/csv?lang=en&timezone=America%2FLos_Angeles&use_labels=true&delimiter=%3B"
#' parks <- read_clean_van_data(parks_url)
#'
#' art_url <- "https://opendata.vancouver.ca/api/explore/v2.1/catalog/datasets/public-art/exports/csv?lang=en&timezone=America%2FLos_Angeles&use_labels=true&delimiter=%3B"
#' art <- read_clean_van_data(art_url)
#'
#' @export
read_clean_van_data <- function(url, delim = ";") { 
# This function takes a URL to a Vancouver Open Data CSV and reads it in
# using the specified delimiter (default ";"), then cleans the column names.
  readr::read_delim(url, delim = delim) |>
    janitor::clean_names()
}

```

## Exercise 4: testing

rubric={accuracy:18}

Sample variance of data generated from a normal/Gaussian distribution is
defined as:

$variance = \frac{\Sigma{(x-mean)^2}}{n-1}$

where $mean$ is the mean of our observations, $x$ is each individual
observation, and $n$ is the number of observations.

Your task is to use test driven development to write a function that
calculates the variance from scratch (*i.e.*, do not use the `var`
function in R). Your function should take in a vector, and return a
vector of length 1. Make sure you use defensive programming so that your
function will fail early (and provides useful error messages) if the
user provides incorrect inputs (e.g., lists, data frames, etc). Use
{testthat} statements to check the correctness of your function on
tractable edge cases, as well as to check that your function handles
exceptions as expected.

*Hint - you likely need to avoid using {tidyverse} functions in your
solution as we will not learn how to write functions with them until
next week (they are a little trickier to program with due to their
unquoted column names).*

```{r tags=c()}

library(testthat)

variance_from_scratch <- function(x) {

  # defensive programming
  if (!is.numeric(x) || is.list(x) || is.data.frame(x)) {
    stop("Input must be a numeric vector.")
  }

  if (length(x) < 2) {
    stop("Input vector must have at least two numeric values.")
  }

  # compute sample mean
  m <- sum(x) / length(x)

  # numerator: sum of squared deviations
  numerator <- sum((x - m)^2)

  # denominator: n - 1 (sample variance)
  variance <- numerator / (length(x) - 1)

  return(variance)
}


test_that("variance_from_scratch works on simple numeric vectors", {
  expect_equal(variance_from_scratch(c(1, 2, 3)), 1)
  expect_equal(variance_from_scratch(c(5, 5, 5)), 0)
})

test_that("variance_from_scratch handles length-1 or length-0 inputs", {
  expect_error(variance_from_scratch(5))
  expect_error(variance_from_scratch(numeric(0)))
})

test_that("variance_from_scratch throws errors for invalid inputs", {
  expect_error(variance_from_scratch("hello"))
  expect_error(variance_from_scratch(list(1, 2, 3)))
  expect_error(variance_from_scratch(data.frame(x = 1:3)))
})

```

```{r}

variance_from_scratch(list(1, 2, 3))
```

## Exercise 5: (Challenging)

rubric={accuracy:5}

**Warning: This exercise is challenging and could be time-consuming.
Please only attempt if you find yourself finishing the assignment early
and you want a bit more of a challenge.**

We're going to be working with a data set from Kaggle to further explore
the {purrr} `map*` functions. This data was collected under the
instructions from Madrid's City Council and is publicly available on
their website. It is named `madrid_pollution.tsv` and is available here
<https://github.com/UBC-DSCI/dsci-100-assets/blob/master/2019-fall/materials/worksheet_03/data/madrid_pollution.csv?raw=true>.
This data includes daily and hourly measurements of air quality from
2001 to 2006. Pollutants are categorized based on their chemical
properties. More information about this data set can be found
[here](https://www.kaggle.com/decide-soluciones/air-quality-madrid).

In this exercise we want you to use create a subset of this data frame
called that contains only the records for the year 2006, and only the
columns with the pollutant values. Then we want you to use a {purrr}
`map*` function and a standard error function (that you write yourself)
to obtain the standard errors for each pollutant in 2006 stored as a
tibble.

The standard error of a normal distribution is defined as the standard
deviation divided by the square root of the number of observations:

$$se = \frac{sd}{\sqrt{n}}$$

There is no function for this in R, so for this question you need to
write this yourself. Be sure to also write tests for your function to
prove that it works as expected.

```{r tags=c()}

```

> Note - there is a new {tidyverse} function, `across`, that is also
> useful for applying a function across columns (docs:
> <https://dplyr.tidyverse.org/reference/across.html>), however we focus
> on teaching `map_*` in MDS as it is more general. Feel free to use
> either in future if the use of `map_*` is not specified.

## Exercise 6: (Optional)

rubric={accuracy:0}

Ask a large language model (LLM) to complete exercise 4 for you (note -
in exercise 4 you are expected to first answer it on your own, without a
the help of an LLM).

Share here your prompts and the solutions. Did you see any improvements
to your solution? Did it make any mistakes?

*Type your answer here, replacing this text.*

Congratulations! You are done the lab!!! Pat yourself on the back, and
submit your lab to **GitHub** and Gradescope! Make sure you have 3 Git
commits!
